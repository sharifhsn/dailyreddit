{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get your daily Reddit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "import openai\n",
    "from wandb.integration.openai import autolog\n",
    "from getpass import getpass\n",
    "from rich.markdown import Markdown\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ii. Initialize environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\n\")\n",
    "assert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn't look like a valid OpenAI API key\"\n",
    "print(\"OpenAI API key configured\")\n",
    "\n",
    "if os.getenv(\"REDDIT_CLIENT_ID\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"REDDIT_CLIENT_ID\"] = getpass(\"Paste your Reddit client ID from: https://old.reddit.com/prefs/apps/\\n\")\n",
    "print(\"Reddit client ID configured\")\n",
    "\n",
    "if os.getenv(\"REDDIT_CLIENT_SECRET\") is None:\n",
    "  if any(['VSCODE' in x for x in os.environ.keys()]):\n",
    "    print('Please enter password in the VS Code prompt at the top of your VS Code window!')\n",
    "  os.environ[\"REDDIT_CLIENT_SECRET\"] = getpass(\"Paste your Reddit client secret from: https://old.reddit.com/prefs/apps/\\n\")\n",
    "print(\"Reddit client secret configured\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"dailyreddit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Initialize `wandb` logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_name: dev\n",
    "def log():\n",
    "    autolog.enable({\"project\": \"dailyreddit\", \"job_type\": \"dev\"})\n",
    "log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the daily Reddit\n",
    "\n",
    "The document base for this application is the top Reddit posts of the day. This data is accessed through the `PRAW` API. \n",
    "\n",
    "Caching is implemented for each post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Tuple\n",
    "import time\n",
    "import praw\n",
    "from praw import Reddit\n",
    "from praw.models import Subreddit, Submission, Comment\n",
    "import pickle\n",
    "\n",
    "reddit: Reddit = praw.Reddit(\n",
    "    client_id = os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "    client_secret = os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "    password = os.environ[\"REDDIT_PASSWORD\"],\n",
    "    user_agent = os.environ[\"REDDIT_USER_AGENT\"],\n",
    "    username = os.environ[\"REDDIT_USERNAME\"],\n",
    ")\n",
    "\n",
    "if not os.path.exists(\"cache\"):\n",
    "    os.mkdir(\"cache\")\n",
    "\n",
    "submissions_cache_file: str = \"cache/submissions_cache.pkl\"\n",
    "\n",
    "def load_cache(cache: str):\n",
    "    try:\n",
    "        with open(cache, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        return None\n",
    "    \n",
    "def save_cache(cache: str, data: Any):\n",
    "    with open(cache, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "def format_comments(comments: List[Comment], depth: int = 0) -> str:\n",
    "    formatted_comments: str = \"\"\n",
    "    for comment in comments:\n",
    "        comment_text: str = comment.body.replace(\"\\n\", \" \")\n",
    "        formatted_comment: str = f\"{'  ' * depth}- {comment_text.replace(' - ', ' _ ').replace('--', 'â€”')}\\n\"\n",
    "        formatted_comments += formatted_comment\n",
    "        if comment.replies:\n",
    "            formatted_comments += format_comments(comment.replies, depth + 1)\n",
    "    return formatted_comments\n",
    "\n",
    "submissions_cache: Dict[str, Any] = load_cache(submissions_cache_file)\n",
    "\n",
    "if not submissions_cache or (time.time() - submissions_cache.get(\"timestamp\", 0)) > 7200:\n",
    "    # clear cache\n",
    "    [os.remove(os.path.join(\"cache\", file)) for file in os.listdir(\"cache\")]\n",
    "\n",
    "    # get subreddit and top posts\n",
    "    subreddit: Subreddit = reddit.subreddit(\"nba\")\n",
    "    top_posts: List[Submission] = subreddit.top(time_filter = \"day\", limit = 25)\n",
    "\n",
    "    # save posts to cache\n",
    "    save_cache(submissions_cache_file, {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"posts\": top_posts\n",
    "    })\n",
    "\n",
    "\n",
    "    # create a new markdown file for each post\n",
    "    for post in top_posts:\n",
    "        post_md: str = \"\"\n",
    "\n",
    "        post.comments.replace_more(limit=1)\n",
    "        comments: List[Comment] = post.comments\n",
    "        post_md += format_comments(comments)\n",
    "        \n",
    "\n",
    "        sanitized_post_title = \"\".join(c if c.isalnum() or c in (\" \", \"-\", \"_\") else \"_\" for c in post.title).strip(\"_\")[:250]\n",
    "        with open(f\"cache/{sanitized_post_title}.md\", \"w\", encoding = \"utf-8\") as f:\n",
    "            f.write(post_md)\n",
    "else:\n",
    "    top_posts: List[Submission] = submissions_cache.get(\"posts\", [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process data\n",
    "\n",
    "We have stored data as Markdown files, now we want to construct each thread as its own document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain_community.document_loaders.telegram import text_to_docs\n",
    "import re\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "MODEL_NAME: str = \"text-davinci-003\"\n",
    "\n",
    "documents = []\n",
    "\n",
    "markdown_files = glob.glob(os.path.join(\"cache\", \"*.md\"))\n",
    "for markdown_file in markdown_files:\n",
    "    with open(markdown_file, \"r\", encoding = \"utf-8\") as file:\n",
    "        documents.append(Document(page_content=file.read(), metadata={\"source\": markdown_file}))\n",
    "\n",
    "document_sections: List[Document] = []\n",
    "\n",
    "pattern = r\"(\\n\\s*)?- (.*?)(?=\\s*\\n\\s*-\\s+|$)\"\n",
    "for document in documents:\n",
    "    matches: List[Tuple[str, str]] = re.findall(pattern, document.page_content, re.DOTALL)\n",
    "    threads: List[Document] = []\n",
    "    current_thread: List[str] = [document.metadata[\"source\"][6:-3]]\n",
    "    current_depth: int = 0\n",
    "\n",
    "    for whitespace, comment_text in matches:\n",
    "        depth: int = len(whitespace) // 2 if whitespace else 0\n",
    "        #print(f\"{'  ' * depth}- {comment_text}\")\n",
    "\n",
    "        if depth == current_depth:\n",
    "            # depth is as expected, continue thread\n",
    "            current_thread.append(comment_text)\n",
    "            current_depth += 1\n",
    "        elif depth < current_depth:\n",
    "            # thread has finished, and this next comment is a retreat\n",
    "            threads.append(Document(page_content = \"\\n\".join(deepcopy(current_thread)), metadata = document.metadata))\n",
    "            while depth < current_depth:\n",
    "                current_thread.pop()\n",
    "                current_depth -= 1\n",
    "            current_thread.append(comment_text)\n",
    "            current_depth += 1\n",
    "        else:\n",
    "            # this should be unreachable\n",
    "            raise Exception(\n",
    "                f\"depth exceeds current depth, malformed data. current_depth: {current_depth}, depth: {depth}, comment: {comment_text}\"\n",
    "            )\n",
    "    \n",
    "    document_sections.extend(threads)\n",
    "\n",
    "for document in document_sections:\n",
    "    print(\"Thread:\")\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed documents in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "embeddings: OpenAIEmbeddings = OpenAIEmbeddings()\n",
    "db: Chroma = Chroma().from_documents(document_sections, embeddings)\n",
    "\n",
    "retriever: VectorStoreRetriever = db.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "query: str = \"How do people feel about Draymond Green still being in the league?\"\n",
    "docs: List[Document] = retriever.get_relevant_documents(query)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template: str = \"\"\"Use the opinions expressed in the following Reddit comments to answer the question at the end.\n",
    "Answer questions with the perspective and tone shown in the comments.\n",
    "Use similar slang and vocabulary to the comments.\n",
    "The first line of each paragraph is the title of the Reddit post, and the following lines are a Reddit thread that has subsequent comments.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT: PromptTemplate = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "context: str = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "prompt = PROMPT.format(context = context, question = query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm: OpenAI = OpenAI()\n",
    "response: str = llm.invoke(prompt)\n",
    "\n",
    "Markdown(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
